{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e522dd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from peft import PeftModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec97e52",
   "metadata": {},
   "source": [
    "### Load datasets + prepare evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d95960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 3200 passages\n",
      "Eval QA: 100 questions\n",
      "Sample QA: {'question': 'Was Abraham Lincoln the sixteenth President of the United States?', 'answer': 'yes', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load corpus for retrieval (same as before)\n",
    "ds_corpus = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
    "passages_ds = ds_corpus[\"passages\"]\n",
    "original_texts = [ex[\"passage\"] for ex in passages_ds]\n",
    "\n",
    "# Load QA dataset for evaluation (we'll use this as gold standard)\n",
    "ds_qa = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
    "qa_test = ds_qa[\"test\"].select(range(100))  # 100 examples for eval\n",
    "\n",
    "print(f\"Corpus: {len(original_texts)} passages\")\n",
    "print(f\"Eval QA: {len(qa_test)} questions\")\n",
    "print(\"Sample QA:\", qa_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e0415",
   "metadata": {},
   "source": [
    "###  Advanced Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938c08c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking strategies ready!\n"
     ]
    }
   ],
   "source": [
    "def chunk_fixed_size(texts, chunk_size=200, overlap=50):\n",
    "    \"\"\"Fixed-size chunking with overlap\"\"\"\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = \" \".join(words[i:i+chunk_size])\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def chunk_by_sentences(texts, max_sentences=5):\n",
    "    \"\"\"Chunk by sentence boundaries\"\"\"\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        sentences = sent_tokenize(text)\n",
    "        for i in range(0, len(sentences), max_sentences):\n",
    "            chunk = \" \".join(sentences[i:i+max_sentences])\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def chunk_semantic(texts, model_e, threshold=0.7):\n",
    "    \"\"\"Simple semantic chunking (group similar sentences)\"\"\"\n",
    "    all_chunks = []\n",
    "    for text in texts:\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) <= 3:\n",
    "            all_chunks.append(text)\n",
    "            continue\n",
    "            \n",
    "        sent_embs = model_e.encode(sentences, convert_to_numpy=True)\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        \n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity([sent_embs[0]], [sent_embs[i]])[0][0]\n",
    "            if sim > threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentences[i]]\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        all_chunks.extend(chunks)\n",
    "    return all_chunks\n",
    "\n",
    "print(\"Chunking strategies ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edf379",
   "metadata": {},
   "source": [
    "### Load Embedding Models + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c289dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 723.58it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "c:\\Users\\arist\\miniconda3\\envs\\random\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arist\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:00<00:00, 786.34it/s, Materializing param=classifier.weight]                                    \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense + Reranker loaded\n"
     ]
    }
   ],
   "source": [
    "# Dense retriever\n",
    "embed_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_e = SentenceTransformer(embed_name)\n",
    "\n",
    "# Reranker (cross-encoder)\n",
    "reranker_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(reranker_name)\n",
    "\n",
    "print(\"Dense + Reranker loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14917e72",
   "metadata": {},
   "source": [
    "### Build Multiple Indexes (Chunking Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3503b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing original: 1000 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:01<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing fixed_200: 538 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing sentences: 596 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 29.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All indexes built!\n"
     ]
    }
   ],
   "source": [
    "def build_indexes(texts_list, model_e):\n",
    "    \"\"\"Build FAISS + BM25 indexes for different chunking strategies\"\"\"\n",
    "    indexes = {}\n",
    "    \n",
    "    for name, texts in texts_list.items():\n",
    "        print(f\"Indexing {name}: {len(texts)} chunks...\")\n",
    "        \n",
    "        # Dense embeddings + FAISS\n",
    "        embs = model_e.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "        dim = embs.shape[1]\n",
    "        faiss_idx = faiss.IndexFlatIP(dim)\n",
    "        faiss_idx.add(embs)\n",
    "        \n",
    "        # BM25 (lexical)\n",
    "        tokenized_texts = [nltk.word_tokenize(t.lower()) for t in texts]\n",
    "        bm25 = BM25Okapi(tokenized_texts)\n",
    "        \n",
    "        indexes[name] = {\n",
    "            'texts': texts,\n",
    "            'faiss': faiss_idx,\n",
    "            'bm25': bm25,\n",
    "            'tokenized': tokenized_texts,\n",
    "            'embs': embs\n",
    "        }\n",
    "    return indexes\n",
    "\n",
    "# Create different chunking strategies\n",
    "chunk_strategies = {\n",
    "    'original': original_texts[:1000],  # Use subset for speed\n",
    "    'fixed_200': chunk_fixed_size(original_texts[:500], 200, 50),\n",
    "    'sentences': chunk_by_sentences(original_texts[:500]),\n",
    "}\n",
    "\n",
    "# Build all indexes\n",
    "indexes = build_indexes(chunk_strategies, model_e)\n",
    "print(\"All indexes built!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca26ae3",
   "metadata": {},
   "source": [
    "### Advanced Multi-stage Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78543c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, index_data, k_final=5, k_bm25=50, k_dense=20):\n",
    "    \"\"\"BM25 â†’ Dense â†’ Rerank pipeline (FIXED)\"\"\"\n",
    "    texts, faiss_idx, bm25, tokenized = index_data['texts'], index_data['faiss'], index_data['bm25'], index_data['tokenized']\n",
    "    \n",
    "    # Stage 1: BM25 (lexical)\n",
    "    tokenized_query = nltk.word_tokenize(query.lower())\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_idx = np.argsort(bm25_scores)[-min(k_bm25, len(bm25_scores)):]  # ðŸ›¡ï¸ Safety\n",
    "    \n",
    "    if len(top_bm25_idx) == 0:\n",
    "        return []  # No candidates\n",
    "    \n",
    "    # Stage 2: Dense retrieval on BM25 candidates\n",
    "    candidate_texts = [texts[i] for i in top_bm25_idx]\n",
    "    q_emb = model_e.encode([query], normalize_embeddings=True)\n",
    "    k_search = min(k_dense, len(top_bm25_idx))\n",
    "    dense_scores, dense_idx = faiss_idx.search(q_emb, k_search)\n",
    "    dense_idx = dense_idx[0]\n",
    "    \n",
    "    # ðŸ›¡ï¸ FIX: Clamp dense_idx to valid BM25 candidates\n",
    "    valid_dense_idx = []\n",
    "    for doc_idx in dense_idx:\n",
    "        if doc_idx < len(top_bm25_idx):\n",
    "            valid_dense_idx.append(doc_idx)\n",
    "        else:\n",
    "            # Fallback to best BM25\n",
    "            valid_dense_idx.append(0)\n",
    "    \n",
    "    # Combine scores (BM25 + dense)\n",
    "    hybrid_scores = []\n",
    "    for i, clamped_idx in enumerate(valid_dense_idx[:k_dense]):\n",
    "        bm25_score = bm25_scores[top_bm25_idx[clamped_idx]]\n",
    "        dense_score = dense_scores[0][i]\n",
    "        hybrid_score = 0.4 * bm25_score + 0.6 * dense_score\n",
    "        hybrid_scores.append((hybrid_score, candidate_texts[clamped_idx]))\n",
    "    \n",
    "    # Stage 3: Rerank top candidates\n",
    "    top_hybrid = sorted(hybrid_scores, reverse=True)[:10]\n",
    "    if len(top_hybrid) > 1:\n",
    "        pairs = [(query, text) for _, text in top_hybrid]\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        top_final = [(rerank_scores[i], text) for i, (_, text) in enumerate(top_hybrid)]\n",
    "    else:\n",
    "        top_final = top_hybrid\n",
    "    \n",
    "    return sorted(top_final, reverse=True)[:k_final]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161ad4e",
   "metadata": {},
   "source": [
    "### Improved RAG Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597c8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced prompting ready!\n"
     ]
    }
   ],
   "source": [
    "def build_advanced_prompt(query, retrieved_passages, strategy_name=\"\"):\n",
    "    \"\"\"Advanced RAG prompt with citation, refusal, and CoT\"\"\"\n",
    "    ctx_text = \"\\n\\n\".join([f\"[C{i+1}] {text[:400]}...\" for i, (_, text) in enumerate(retrieved_passages)])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert research assistant. Answer using ONLY the provided context.\n",
    "\n",
    "CONTEXT (retrieved with {strategy_name}):\n",
    "{ctx_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use ONLY information from the context above\n",
    "2. If context lacks sufficient information, say \"I don't know based on provided context\"\n",
    "3. Cite sources using [C1], [C2], etc. format\n",
    "4. Provide a concise, accurate answer\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"Advanced prompting ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8267d7c",
   "metadata": {},
   "source": [
    "### Reload Mistral LoRA + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82777f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 291/291 [01:15<00:00,  3.86it/s, Materializing param=model.norm.weight]                              \n",
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'max_new_tokens', 'do_sample', 'repetition_penalty', 'top_p'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "# Same as before but with better generation params\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(base_model, \"./mistral-rag-mini-lora\")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,  # Lower for consistency\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    result = gen(prompt, return_full_text=False)[0][\"generated_text\"].strip()\n",
    "    return result.split(\"ANSWER:\")[-1].strip() if \"ANSWER:\" in result else result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae0590",
   "metadata": {},
   "source": [
    "### End-to-End Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0683a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY: Who was Ulysses S. Grant?\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL (top-1 retrieval score: -9.232)\n",
      "CONTEXT: Blaise Pascal ( ), (June 19 1623   August 19 1662) was a French mathematician, physicist, and religious philosopher. He was a child prodigy who was educated by his father. Pascal's earliest work was i...\n",
      "ANSWER: [(np.float32(-9.231781), \"Blaise Pascal ( ), (June 19 1623   August 19 1662) was a French mathematician, physicist, and religious philosopher. He was a child prodigy who was educated by his father. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli. Pascal also wrote in defense of the scientific method.\"), (np.float32(-9.231781), \"Blaise Pascal ( ), (June 19 1623   August 19 1662) was a French mathematician, physicist, and religious philosopher. He was a child prodigy who was educated by his father. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli. Pascal also wrote in defense of the scientific method.\"), (np.float32(-9.231781), \"Blaise Pascal ( ), (June 19 1623   August 19 1662) was a French mathematician, physicist, and religious philosopher. He was a child prodigy who was educated by his father. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the construction of mechanical calculators, the study of fluids, and clarified the concepts of pressure and vacuum by generalizing the work of Evangelista Torricelli. Pascal also wrote in defense of the scientific method.\")]\n",
      "--------------------------------------------------\n",
      "\n",
      "FIXED_200 (top-1 retrieval score: -9.597)\n",
      "CONTEXT: Possibly the most notable criminal trial of Lincoln's career as a lawyer came in 1858, when he defended William \"Duff\" Armstrong, who has been charged with murder. The case became famous for Lincoln's...\n",
      "ANSWER: [(np.float32(-9.597066), 'Possibly the most notable criminal trial of Lincoln\\'s career as a lawyer came in 1858, when he defended William \"Duff\" Armstrong, who has been charged with murder. The case became famous for Lincoln\\'s use of judicial notice--a rare tactic at that time--to show that an eyewitness had lied on the stand. After the witness testified to having seen the crime by moonlight, Lincoln produced a Farmers\\' Almanac to show that the moon on that date was at such a low angle that it could not have provided enough illumination to see anything clearly. Based almost entirely on this evidence, Armstrong was acquitted. Donald (1995), 150-51'), (np.float32(-9.597066), 'Possibly the most notable criminal trial of Lincoln\\'s career as a lawyer came in 1858, when he defended William \"Duff\" Armstrong, who has been charged with murder. The case became famous for Lincoln\\'s use of judicial notice--a rare tactic at that time--to show that an eyewitness had lied on the stand. After the witness testified to having seen the crime by moonlight, Lincoln produced a Farmers\\' Almanac to show that the moon on that date was at such a low angle that it could not have provided enough illumination to see anything clearly. Based almost entirely on this evidence, Armstrong was acquitted. Donald (1995), 150-51'), (np.float32(-9.597066), 'Possibly the most notable criminal trial of Lincoln\\'s career as a lawyer came in 1858, when he defended William \"Duff\" Armstrong, who has been charged with murder. The case became famous for Lincoln\\'s use of judicial notice--a rare tactic at that time--to show that an eyewitness had lied on the stand. After the witness testified to having seen the crime by moonlight, Lincoln produced a Farmers\\' Almanac to show that the moon on that date was at such a low angle that it could not have provided enough illumination to see anything clearly. Based almost entirely on this evidence, Armstrong was acquitted. Donald (1995), 150-51')]\n",
      "--------------------------------------------------\n",
      "\n",
      "SENTENCES (top-1 retrieval score: -11.127)\n",
      "CONTEXT: By the end of October 1651 a truce had been reached between brother and sister: in return for a respectable annual stipend, Jacqueline signed over inheritance to her brother. (Their eldest sister Gilb...\n",
      "ANSWER: [(np.float32(-11.126551), 'By the end of October 1651 a truce had been reached between brother and sister: in return for a respectable annual stipend, Jacqueline signed over inheritance to her brother. (Their eldest sister Gilberte had already been given her inheritance in the form of a handsome dowry.) On 04 January, Jacqueline left for Port-Royal. On that day, according to Gilberte, \"He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor...\" Jacqueline Pascal, \"Memoir\" p. 87'), (np.float32(-11.126551), 'By the end of October 1651 a truce had been reached between brother and sister: in return for a respectable annual stipend, Jacqueline signed over inheritance to her brother. (Their eldest sister Gilberte had already been given her inheritance in the form of a handsome dowry.) On 04 January, Jacqueline left for Port-Royal. On that day, according to Gilberte, \"He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor...\" Jacqueline Pascal, \"Memoir\" p. 87'), (np.float32(-11.126551), 'By the end of October 1651 a truce had been reached between brother and sister: in return for a respectable annual stipend, Jacqueline signed over inheritance to her brother. (Their eldest sister Gilberte had already been given her inheritance in the form of a handsome dowry.) On 04 January, Jacqueline left for Port-Royal. On that day, according to Gilberte, \"He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor...\" Jacqueline Pascal, \"Memoir\" p. 87')]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def advanced_rag(query, indexes, k=5, show_comparison=False):\n",
    "    \"\"\"Compare all chunking strategies\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for strategy, index_data in indexes.items():\n",
    "        # Multi-stage retrieval\n",
    "        retrieved = hybrid_search(query, index_data, k_final=k)\n",
    "        \n",
    "        # Advanced prompt\n",
    "        prompt = build_advanced_prompt(query, retrieved, strategy)\n",
    "        \n",
    "        # Generate\n",
    "        answer = generate_answer(prompt)\n",
    "        \n",
    "        results[strategy] = {\n",
    "            'retrieved': retrieved,\n",
    "            'prompt': prompt,\n",
    "            'answer': answer\n",
    "        }\n",
    "    \n",
    "    if show_comparison:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for strategy, res in results.items():\n",
    "            print(f\"\\n{strategy.upper()} (top-1 retrieval score: {res['retrieved'][0][0]:.3f})\")\n",
    "            print(f\"CONTEXT: {res['retrieved'][0][1][:200]}...\")\n",
    "            print(f\"ANSWER: {res['retrieved'][:100]}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it!\n",
    "query = \"Who was Ulysses S. Grant?\"\n",
    "results = advanced_rag(query, indexes, k=3, show_comparison=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6da19",
   "metadata": {},
   "source": [
    "### Comprehensive Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b13f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS (across strategies)\n",
      "================================================================================\n",
      "original    : EM=55.0% | R1=0.659 | RL=0.659 | Recall=0.000\n",
      "fixed_200   : EM=50.0% | R1=0.592 | RL=0.592 | Recall=0.050\n",
      "sentences   : EM=45.0% | R1=0.536 | RL=0.536 | Recall=0.000\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Normalize for exact match\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', s.lower().strip())\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return normalize_answer(pred) == normalize_answer(gold)\n",
    "\n",
    "def rouge_score(pred, gold):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(gold, pred)\n",
    "    return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure\n",
    "\n",
    "def evaluate_retrieval(qa_example, retrieved_passages, original_texts):\n",
    "    \"\"\"Does retrieval find relevant passages?\"\"\"\n",
    "    gold_answer = qa_example['answer'].lower()\n",
    "    relevant_chunks = 0\n",
    "    total_chunks = len(retrieved_passages)\n",
    "    \n",
    "    for _, passage in retrieved_passages:\n",
    "        if any(word in passage.lower() for word in gold_answer.split() if len(word) > 3):\n",
    "            relevant_chunks += 1\n",
    "    \n",
    "    recall = relevant_chunks / max(total_chunks, 1)\n",
    "    return recall\n",
    "\n",
    "def full_evaluation(indexes, qa_test, n_eval=20):\n",
    "    \"\"\"Run full evaluation across strategies\"\"\"\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for i in range(n_eval):\n",
    "        qa = qa_test[i]\n",
    "        query = qa['question']\n",
    "        gold = qa['answer']\n",
    "        \n",
    "        strategy_results = advanced_rag(query, indexes, k=3, show_comparison=False)\n",
    "        \n",
    "        for strategy, res in strategy_results.items():\n",
    "            pred = res['answer']\n",
    "            \n",
    "            # QA metrics\n",
    "            em = exact_match(pred, gold)\n",
    "            r1, rL = rouge_score(pred, gold)\n",
    "            \n",
    "            # Retrieval metric\n",
    "            recall = evaluate_retrieval(qa, res['retrieved'], original_texts)\n",
    "            \n",
    "            results[strategy].append({\n",
    "                'em': em,\n",
    "                'rouge1': r1,\n",
    "                'rougeL': rL,\n",
    "                'recall': recall\n",
    "            })\n",
    "    \n",
    "    # Aggregate\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS (across strategies)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    eval_table = []\n",
    "    for strategy, metrics in results.items():\n",
    "        avg_em = np.mean([m['em'] for m in metrics])\n",
    "        avg_r1 = np.mean([m['rouge1'] for m in metrics])\n",
    "        avg_rL = np.mean([m['rougeL'] for m in metrics])\n",
    "        avg_recall = np.mean([m['recall'] for m in metrics])\n",
    "        \n",
    "        eval_table.append([strategy, f\"{avg_em:.1%}\", f\"{avg_r1:.3f}\", f\"{avg_rL:.3f}\", f\"{avg_recall:.3f}\"])\n",
    "        print(f\"{strategy:12}: EM={avg_em:.1%} | R1={avg_r1:.3f} | RL={avg_rL:.3f} | Recall={avg_recall:.3f}\")\n",
    "    \n",
    "    return eval_table\n",
    "\n",
    "# Run evaluation!\n",
    "eval_results = full_evaluation(indexes, qa_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b017b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merouane fact injected into 'merouane_fixed' index\n",
      "New index size: 539\n"
     ]
    }
   ],
   "source": [
    "## Cell 11 â€“ Merouane External Knowledge Test (RAG override)\n",
    "\n",
    "# 1. Inject external fact into ONE chunking strategy\n",
    "merouane_fact = \"The creator of this RAG notebook is named Merouane. Merouane built the fine-tuning pipeline and advanced retrieval system.\"\n",
    "\n",
    "# Add to 'fixed_200' strategy only (to compare with others)\n",
    "indexes['merouane_fixed'] = indexes['fixed_200'].copy()\n",
    "indexes['merouane_fixed']['texts'].append(merouane_fact)\n",
    "\n",
    "# Rebuild index with new fact\n",
    "new_emb = model_e.encode([merouane_fact], normalize_embeddings=True, convert_to_numpy=True)\n",
    "indexes['merouane_fixed']['faiss'].add(new_emb)\n",
    "indexes['merouane_fixed']['tokenized'].append(nltk.word_tokenize(merouane_fact.lower()))\n",
    "\n",
    "print(\"âœ… Merouane fact injected into 'merouane_fixed' index\")\n",
    "print(f\"New index size: {indexes['merouane_fixed']['faiss'].ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0da4bb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY: Who created this RAG notebook?\n",
      "================================================================================\n",
      "\n",
      "ORIGINAL (top-1 retrieval score: -11.264)\n",
      "CONTEXT: In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conducto...\n",
      "ANSWER: [(np.float32(-11.26402), 'In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields due to them cancel. This shielding effect is used in what is now known as a Faraday cage.'), (np.float32(-11.26402), 'In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields due to them cancel. This shielding effect is used in what is now known as a Faraday cage.'), (np.float32(-11.26402), 'In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields due to them cancel. This shielding effect is used in what is now known as a Faraday cage.')]\n",
      "--------------------------------------------------\n",
      "\n",
      "FIXED_200 (top-1 retrieval score: -11.070)\n",
      "CONTEXT: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, ÃƒÂ‰tienne Pascal (1588Ã¢Â€Â“1651), was a local judge and membe...\n",
      "ANSWER: [(np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.'), (np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.'), (np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.')]\n",
      "--------------------------------------------------\n",
      "\n",
      "SENTENCES (top-1 retrieval score: -11.220)\n",
      "CONTEXT: McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chief after releasing his Harrison's Landing Letter, where he offered unsolicited political advice to Li...\n",
      "ANSWER: [(np.float32(-11.219709), \"McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chief after releasing his Harrison's Landing Letter, where he offered unsolicited political advice to Lincoln urging caution in the war effort. McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack. But Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time. In response to his failure, Pope was sent to Minnesota to fight the Sioux.\"), (np.float32(-11.219709), \"McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chief after releasing his Harrison's Landing Letter, where he offered unsolicited political advice to Lincoln urging caution in the war effort. McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack. But Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time. In response to his failure, Pope was sent to Minnesota to fight the Sioux.\"), (np.float32(-11.219709), \"McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chief after releasing his Harrison's Landing Letter, where he offered unsolicited political advice to Lincoln urging caution in the war effort. McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack. But Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time. In response to his failure, Pope was sent to Minnesota to fight the Sioux.\")]\n",
      "--------------------------------------------------\n",
      "\n",
      "MEROUANE_FIXED (top-1 retrieval score: -11.070)\n",
      "CONTEXT: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, ÃƒÂ‰tienne Pascal (1588Ã¢Â€Â“1651), was a local judge and membe...\n",
      "ANSWER: [(np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.'), (np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.'), (np.float32(-11.0702), 'Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinette Begon, at the age of three. His father, Ãƒ\\x89tienne Pascal (1588Ã¢\\x80\\x931651), was a local judge and member of the \"noblesse de robe\", who also had an interest in science and mathematics. Blaise Pascal was brother to Jacqueline Pascal the youngest sibling and Gilberte, the eldest.')]\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "ðŸŽ¯ KEY TEST: Does RAG override model knowledge?\n",
      "====================================================================================================\n",
      "original        | Top context has Merouane: False\n",
      "                  | Context preview: In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior...\n",
      "                  | Answer cites it: False\n",
      "\n",
      "fixed_200       | Top context has Merouane: False\n",
      "                  | Context preview: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinett...\n",
      "                  | Answer cites it: False\n",
      "\n",
      "sentences       | Top context has Merouane: False\n",
      "                  | Context preview: McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chie...\n",
      "                  | Answer cites it: False\n",
      "\n",
      "merouane_fixed  | Top context has Merouane: False\n",
      "                  | Context preview: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinett...\n",
      "                  | Answer cites it: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test the override\n",
    "merouane_query = \"Who created this RAG notebook?\"\n",
    "\n",
    "# Compare ALL strategies (including the one with Merouane)\n",
    "results = advanced_rag(merouane_query, indexes, k=3, show_comparison=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸŽ¯ KEY TEST: Does RAG override model knowledge?\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for strategy, res in results.items():\n",
    "    top_context = res['retrieved'][0][1]\n",
    "    has_merouane = \"Merouane\" in top_context\n",
    "    \n",
    "    print(f\"{strategy:15} | Top context has Merouane: {has_merouane}\")\n",
    "    print(f\"                  | Context preview: {top_context[:100]}...\")\n",
    "    print(f\"                  | Answer cites it: {'Merouane' in res['answer']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22da1f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DEBUG MEROUANE\n",
      "Index size: 539 chunks\n",
      "Merouane fact at exact position: 538\n",
      "\n",
      "ðŸ“Š Top 20 DENSE scores (across ALL chunks):\n",
      "   1. Score= 0.657 | Merouane: True | The creator of this RAG notebook is named Merouane. Merouane built the fine-tuning pipeline and adva...\n",
      "   2. Score= 0.347 | Merouane: False | An early, undated photograph of Grover Cleveland from the Cleveland Family Papers at the New Jersey ...\n",
      "   3. Score= 0.292 | Merouane: False | * Gore Vidal. Lincoln ISBN 0-375-70876-6, a novel....\n",
      "   4. Score= 0.292 | Merouane: False | * Sobel, Robert, Coolidge: An American Enigma (1998), ISBN 0895264102....\n",
      "   5. Score= 0.291 | Merouane: False | Faraday's greatest work was probably with electricity and magnetism. The first experiment which he r...\n",
      "   6. Score= 0.284 | Merouane: False | As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented a...\n",
      "   7. Score= 0.284 | Merouane: False | * Williams, L. Pearce (1971), Faraday: A Biography, Simon and Schuster....\n",
      "   8. Score= 0.283 | Merouane: False | * Coolidge, Calvin. The Autobiography of Calvin Coolidge (1929), ISBN 0944951031....\n",
      "   9. Score= 0.280 | Merouane: False | * Donald, David Herbert. We Are Lincoln Men: Abraham Lincoln and His Friends Simon & Schuster, (2003...\n",
      "  10. Score= 0.279 | Merouane: False | *In 2007, George Pendle wrote The Remarkable Millard Fillmore, a fake biography based on real events...\n",
      "  11. Score= 0.278 | Merouane: False | He invented an early form of what was to become the Bunsen burner, which is used almost universally ...\n",
      "  12. Score= 0.275 | Merouane: False | An early Pascaline on display at the MusÃƒÂ©e des Arts et MÃƒÂ©tiers in the Louvre Museum, Paris....\n",
      "  13. Score= 0.275 | Merouane: False | Pascal studying the cycloid, by Augustin Pajou, 1785, Louvre...\n",
      "  14. Score= 0.269 | Merouane: False | * Ostendorf, Lloyd, and Hamilton, Charles, Lincoln in Photographs: An Album of Every Known Pose, Mor...\n",
      "  15. Score= 0.269 | Merouane: False | only by chance after his death. Oeuvres complÃƒÂ¨tes, 618. This piece is now known as the Memorial. Th...\n",
      "  16. Score= 0.268 | Merouane: False | * Bence Jones, Henry (1870). The Life and Letters of Faraday in 2 vols, Longmans....\n",
      "  17. Score= 0.267 | Merouane: False | * Wilson, Joan Hoff, Herbert Hoover, Forgotten Progressive (1975), ISBN 0316944165....\n",
      "  18. Score= 0.265 | Merouane: False | The observatory of Anders Celsius, from a contemporary engraving....\n",
      "  19. Score= 0.265 | Merouane: False | * Davidson, Hugh M. Blaise Pascal. Boston: Twayne Publishers, 1983....\n",
      "  20. Score= 0.265 | Merouane: False | *Fillmore, a bookworm, found the White House devoid of books and initiated the White House library....\n",
      "\n",
      "ðŸŽ¯ Merouane in top-20 dense? True\n"
     ]
    }
   ],
   "source": [
    "## Cell 12 â€“ DEBUG SAFE: Merouane retrieval (FAISS-proof)\n",
    "\n",
    "def debug_merouane_search(query, index_data):\n",
    "    \"\"\"Debug: search with FULL safety bounds\"\"\"\n",
    "    texts = index_data['texts']\n",
    "    faiss_idx = index_data['faiss']\n",
    "    bm25 = index_data['bm25']\n",
    "    \n",
    "    # 1. Find Merouane position\n",
    "    merouane_pos = next((i for i, t in enumerate(texts) if \"Merouane\" in t), None)\n",
    "    print(f\"ðŸ” DEBUG MEROUANE\")\n",
    "    print(f\"Index size: {len(texts)} chunks\")\n",
    "    print(f\"Merouane fact at exact position: {merouane_pos}\")\n",
    "    \n",
    "    if merouane_pos is None:\n",
    "        print(\"âŒ Merouane fact NOT found in texts!\")\n",
    "        return\n",
    "    \n",
    "    # 2. BM25 top 100 (SAFE)\n",
    "    tokenized_query = nltk.word_tokenize(query.lower())\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_bm25_idx = np.argsort(bm25_scores)[-min(100, len(bm25_scores)):]\n",
    "    \n",
    "    # 3. FULL FAISS search on ALL index (not just BM25 candidates)\n",
    "    q_emb = model_e.encode([query], normalize_embeddings=True)\n",
    "    dense_scores, dense_idx = faiss_idx.search(q_emb, 20)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Top 20 DENSE scores (across ALL chunks):\")\n",
    "    merouane_in_top20 = False\n",
    "    for i in range(min(20, len(dense_idx[0]))):\n",
    "        global_idx = dense_idx[0][i]  # Index dans l'index FAISS complet\n",
    "        if global_idx >= len(texts):\n",
    "            print(f\"  {i+1:2d}. INVALID IDX {global_idx} (skip)\")\n",
    "            continue\n",
    "            \n",
    "        score = dense_scores[0][i]\n",
    "        text_preview = texts[global_idx][:100]\n",
    "        is_merouane = global_idx == merouane_pos\n",
    "        \n",
    "        print(f\"  {i+1:2d}. Score={score:6.3f} | Merouane: {is_merouane} | {text_preview}...\")\n",
    "        if is_merouane:\n",
    "            merouane_in_top20 = True\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Merouane in top-20 dense? {merouane_in_top20}\")\n",
    "\n",
    "# Run debug\n",
    "debug_merouane_search(\"Who created this RAG notebook?\", indexes['merouane_fixed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9980ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MEROUANE trouvÃ©! Position 1/50, score=0.657\n",
      "\n",
      "ðŸ¤– MISTRAL ANSWER: Merouane\n",
      "âœ… RAG INJECTION SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "## Cell 13 â€“ FORCE Merouane test (sans hybrid complexitÃ©)\n",
    "\n",
    "def simple_merouane_test():\n",
    "    index_data = indexes['merouane_fixed']\n",
    "    query = \"Who created this RAG notebook?\"\n",
    "    \n",
    "    # Direct FAISS search (k=50, pas de BM25)\n",
    "    q_emb = model_e.encode([query], normalize_embeddings=True)\n",
    "    scores, indices = index_data['faiss'].search(q_emb, 50)\n",
    "    \n",
    "    # Find Merouane\n",
    "    merouane_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(index_data['texts']):\n",
    "            text = index_data['texts'][idx]\n",
    "            if \"Merouane\" in text:\n",
    "                merouane_chunks.append((scores[0][i], text))\n",
    "                print(f\"âœ… MEROUANE trouvÃ©! Position {i+1}/50, score={scores[0][i]:.3f}\")\n",
    "                break\n",
    "    \n",
    "    if merouane_chunks:\n",
    "        # Build prompt avec Merouane\n",
    "        ctx = \"\\n\\n\".join([f\"[C{i+1}] {t[:300]}...\" for i, (_, t) in enumerate(merouane_chunks)])\n",
    "        prompt = f\"\"\"CONTEXT:\n",
    "{ctx}\n",
    "\n",
    "QUESTION: {query}\n",
    "ANSWER using ONLY context:\"\"\"\n",
    "        \n",
    "        answer = generate_answer(prompt)\n",
    "        print(f\"\\nðŸ¤– MISTRAL ANSWER: {answer}\")\n",
    "        print(\"âœ… RAG INJECTION SUCCESS!\")\n",
    "    else:\n",
    "        print(\"âŒ Merouane not in top-50 dense\")\n",
    "\n",
    "simple_merouane_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9c8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ðŸŽ¯ TEST FINAL: RAG Override (k=20 pour toutes stratÃ©gies)\n",
      "====================================================================================================\n",
      "original        | Merouane rank: N/A/10 | In top-5: False\n",
      "                  | Top-1 preview: In his work on static electricity, Faraday demonstrated that the charge only resided on the exterior...\n",
      "                  | Answer: False â†’ Michael Faraday...\n",
      "--------------------------------------------------------------------------------\n",
      "fixed_200       | Merouane rank: N/A/10 | In top-5: False\n",
      "                  | Top-1 preview: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinett...\n",
      "                  | Answer: False â†’ Blaise Pascal...\n",
      "--------------------------------------------------------------------------------\n",
      "sentences       | Merouane rank: N/A/10 | In top-5: False\n",
      "                  | Top-1 preview: McClellan, a lifelong Democrat who was temperamentally conservative, was relieved as general-in-chie...\n",
      "                  | Answer: False â†’ George B. McClellan...\n",
      "--------------------------------------------------------------------------------\n",
      "merouane_fixed  | Merouane rank: N/A/10 | In top-5: False\n",
      "                  | Top-1 preview: Born in Clermont-Ferrand, in the Auvergne region of France, Blaise Pascal lost his mother, Antoinett...\n",
      "                  | Answer: False â†’ Blaise Pascal...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Cell 14 â€“ Test FINAL: Toutes stratÃ©gies avec k=20\n",
    "\n",
    "merouane_query = \"Who created this RAG notebook?\"\n",
    "\n",
    "# Test TOUTES les stratÃ©gies avec k=20 (pour attraper Merouane)\n",
    "results_k20 = advanced_rag(merouane_query, indexes, k=20, show_comparison=False)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"ðŸŽ¯ TEST FINAL: RAG Override (k=20 pour toutes stratÃ©gies)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for strategy, res in results_k20.items():\n",
    "    # VÃ©rif top-5 (Merouane devrait Ãªtre dedans avec k=20)\n",
    "    top_contexts = [t[1] for t in res['retrieved'][:5]]\n",
    "    has_merouane = any(\"Merouane\" in ctx for ctx in top_contexts)\n",
    "    merouane_pos = next((i+1 for i, (_, ctx) in enumerate(res['retrieved']) if \"Merouane\" in ctx), \"N/A\")\n",
    "    \n",
    "    print(f\"{strategy:15} | Merouane rank: {merouane_pos:2}/{len(res['retrieved'])} | In top-5: {has_merouane}\")\n",
    "    print(f\"                  | Top-1 preview: {res['retrieved'][0][1][:100]}...\")\n",
    "    print(f\"                  | Answer: {'Merouane' in res['answer']} â†’ {res['answer'][:80]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dbdad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” VÃ‰RIFICATION des indexes:\n",
      "original        | Chunks total: 1000 | Merouane chunks: 0\n",
      "fixed_200       | Chunks total:  539 | Merouane chunks: 1\n",
      "sentences       | Chunks total:  596 | Merouane chunks: 0\n",
      "merouane_fixed  | Chunks total:  539 | Merouane chunks: 1\n",
      "\n",
      "âœ… REBUILD merouane_fixed_v2:\n",
      "  New size: 540 chunks\n",
      "  Merouane at: 538\n",
      "  FAISS ntotal: 540\n"
     ]
    }
   ],
   "source": [
    "## Cell 16 â€“ VÃ‰RIF + RE-INJECT Merouane (100% sÃ»r)\n",
    "\n",
    "print(\"ðŸ” VÃ‰RIFICATION des indexes:\")\n",
    "for name, data in indexes.items():\n",
    "    texts = data['texts']\n",
    "    merouane_count = sum(1 for t in texts if \"Merouane\" in t)\n",
    "    print(f\"{name:15} | Chunks total: {len(texts):4d} | Merouane chunks: {merouane_count}\")\n",
    "\n",
    "# REBUILD merouane_fixed proprement\n",
    "merouane_fact = \"The creator of this RAG notebook is named Merouane. Merouane built the fine-tuning pipeline and advanced retrieval system.\"\n",
    "\n",
    "# Copie + ajout PROPRE\n",
    "new_texts = indexes['fixed_200']['texts'].copy()\n",
    "new_texts.append(merouane_fact)\n",
    "\n",
    "new_embs = model_e.encode(new_texts, normalize_embeddings=True, batch_size=64)\n",
    "dim = new_embs.shape[1]\n",
    "new_faiss = faiss.IndexFlatIP(dim)\n",
    "new_faiss.add(new_embs)\n",
    "\n",
    "new_tokenized = [nltk.word_tokenize(t.lower()) for t in new_texts]\n",
    "new_bm25 = BM25Okapi(new_tokenized)\n",
    "\n",
    "indexes['merouane_fixed_v2'] = {\n",
    "    'texts': new_texts,\n",
    "    'faiss': new_faiss,\n",
    "    'bm25': new_bm25,\n",
    "    'tokenized': new_tokenized,\n",
    "    'embs': new_embs\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… REBUILD merouane_fixed_v2:\")\n",
    "print(f\"  New size: {len(new_texts)} chunks\")\n",
    "print(f\"  Merouane at: {new_texts.index(merouane_fact)}\")\n",
    "print(f\"  FAISS ntotal: {new_faiss.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a8cbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ðŸŽ¯ TEST FINAL v2 (avec rebuild)\n",
      "====================================================================================================\n",
      "original        | Merouane rank: N/A | Top5: False\n",
      "                  | Answer: âŒ Pas trouvÃ©\n",
      "                  | 'Michael Faraday...'\n",
      "--------------------------------------------------------------------------------\n",
      "fixed_200       | Merouane rank: N/A | Top5: False\n",
      "                  | Answer: âŒ Pas trouvÃ©\n",
      "                  | 'Blaise Pascal...'\n",
      "--------------------------------------------------------------------------------\n",
      "sentences       | Merouane rank: N/A | Top5: False\n",
      "                  | Answer: âŒ Pas trouvÃ©\n",
      "                  | 'The RAG...'\n",
      "--------------------------------------------------------------------------------\n",
      "merouane_fixed_v2 | Merouane rank: N/A | Top5: False\n",
      "                  | Answer: âŒ Pas trouvÃ©\n",
      "                  | 'Abraham Lincoln...'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Cell 17 â€“ Test AVEC merouane_fixed_v2\n",
    "\n",
    "merouane_query = \"Who created this RAG notebook?\"\n",
    "test_indexes = {k: v for k, v in indexes.items() if k != 'merouane_fixed'}\n",
    "test_indexes['merouane_fixed_v2'] = indexes['merouane_fixed_v2']\n",
    "\n",
    "results_final = advanced_rag(merouane_query, test_indexes, k=20, show_comparison=False)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"ðŸŽ¯ TEST FINAL v2 (avec rebuild)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for strategy, res in results_final.items():\n",
    "    merouane_pos = next((i+1 for i, (_, ctx) in enumerate(res['retrieved']) if \"Merouane\" in ctx), \"N/A\")\n",
    "    has_merouane_top5 = any(\"Merouane\" in ctx[1] for ctx in res['retrieved'][:5])\n",
    "    \n",
    "    print(f\"{strategy:15} | Merouane rank: {merouane_pos:3} | Top5: {has_merouane_top5}\")\n",
    "    print(f\"                  | Answer: {'âœ… Merouane citÃ©!' if 'Merouane' in res['answer'] else 'âŒ Pas trouvÃ©'}\")\n",
    "    print(f\"                  | '{res['answer'][:60]}...'\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c0815e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merouane dans texts Ã  position: 538\n",
      "Query embedding shape: (1, 384)\n",
      "FAISS search: scores=[0.6573448  0.6573448  0.3474611  0.29217365 0.29168218], indices=[539 538 512 422 506  97  79 138 497 416]\n",
      "\n",
      "Top 5 chunks retrouvÃ©s:\n",
      "  1. Score= 0.657 | Merouane: True | The creator of this RAG notebook is named Merouane. Merouane built the fine-tuni...\n",
      "  2. Score= 0.657 | Merouane: True | The creator of this RAG notebook is named Merouane. Merouane built the fine-tuni...\n",
      "  3. Score= 0.347 | Merouane: False | An early, undated photograph of Grover Cleveland from the Cleveland Family Paper...\n",
      "  4. Score= 0.292 | Merouane: False | * Gore Vidal. Lincoln ISBN 0-375-70876-6, a novel....\n",
      "  5. Score= 0.292 | Merouane: False | * Sobel, Robert, Coolidge: An American Enigma (1998), ISBN 0895264102....\n",
      "\n",
      "ðŸŽ¯ Merouane retrouvÃ©? âœ… OUI\n"
     ]
    }
   ],
   "source": [
    "## Cell 18 â€“ TEST DIRECT FAISS (diag ultime)\n",
    "\n",
    "merouane_query = \"Who created this RAG notebook?\"\n",
    "index_data = indexes['merouane_fixed_v2']\n",
    "\n",
    "# 1. VÃ©rif fait prÃ©sent\n",
    "merouane_pos = next((i for i, t in enumerate(index_data['texts']) if \"Merouane\" in t), None)\n",
    "print(f\"âœ… Merouane dans texts Ã  position: {merouane_pos}\")\n",
    "\n",
    "# 2. Test DIRECT FAISS (pas de BM25)\n",
    "q_emb = model_e.encode([merouane_query], normalize_embeddings=True)\n",
    "print(f\"Query embedding shape: {q_emb.shape}\")\n",
    "\n",
    "scores, indices = index_data['faiss'].search(q_emb, k=10)\n",
    "print(f\"FAISS search: scores={scores[0][:5]}, indices={indices[0]}\")\n",
    "\n",
    "# 3. VÃ©rif chunks retrouvÃ©s\n",
    "print(\"\\nTop 5 chunks retrouvÃ©s:\")\n",
    "for i, idx in enumerate(indices[0][:5]):\n",
    "    if idx < len(index_data['texts']):\n",
    "        text = index_data['texts'][idx]\n",
    "        is_merouane = \"Merouane\" in text\n",
    "        print(f\"  {i+1}. Score={scores[0][i]:6.3f} | Merouane: {is_merouane} | {text[:80]}...\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. INVALID INDEX {idx}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Merouane retrouvÃ©? {'âœ… OUI' if merouane_pos in indices[0][:10] else 'âŒ NON'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbead3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mini index crÃ©Ã© (5 chunks, Merouane #3)\n",
      "Texts: ['Michael Faraday discovered electromagnet...', 'Blaise Pascal invented the calculator....', 'Abraham Lincoln was US president....', 'The creator of this RAG notebook is Mero...', 'George McClellan Civil War general....']\n"
     ]
    }
   ],
   "source": [
    "## Cell 19 â€“ REBUILD MINI (5 chunks pour test)\n",
    "\n",
    "mini_texts = [\n",
    "    \"Michael Faraday discovered electromagnetic induction.\",\n",
    "    \"Blaise Pascal invented the calculator.\",\n",
    "    \"Abraham Lincoln was US president.\",\n",
    "    \"The creator of this RAG notebook is Merouane.\",  # Position 3\n",
    "    \"George McClellan Civil War general.\"\n",
    "]\n",
    "\n",
    "mini_embs = model_e.encode(mini_texts, normalize_embeddings=True)\n",
    "dim = mini_embs.shape[1]\n",
    "mini_faiss = faiss.IndexFlatIP(dim)\n",
    "mini_faiss.add(mini_embs)\n",
    "\n",
    "mini_tokenized = [nltk.word_tokenize(t.lower()) for t in mini_texts]\n",
    "mini_bm25 = BM25Okapi(mini_tokenized)\n",
    "\n",
    "indexes['mini_test'] = {\n",
    "    'texts': mini_texts,\n",
    "    'faiss': mini_faiss,\n",
    "    'bm25': mini_bm25,\n",
    "    'tokenized': mini_tokenized\n",
    "}\n",
    "\n",
    "print(\"âœ… Mini index crÃ©Ã© (5 chunks, Merouane #3)\")\n",
    "print(\"Texts:\", [t[:40]+\"...\" for t in mini_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8ea9e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini test answer: Merouane\n"
     ]
    }
   ],
   "source": [
    "## Test mini\n",
    "mini_results = advanced_rag(\"Who created this RAG notebook?\", {'mini_test': indexes['mini_test']}, k=5)\n",
    "print(\"Mini test answer:\", mini_results['mini_test']['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20d339b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=200) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ðŸŽ¯ PURE DENSE RAG (k=50, sans BM25)\n",
      "====================================================================================================\n",
      "original        | Merouane rank: N/A | Answer: ...\n",
      "fixed_200       | Merouane rank: 1 | Answer: Merouane...\n",
      "sentences       | Merouane rank: N/A | Answer: Michael Faraday...\n",
      "merouane_fixed  | Merouane rank: 1 | Answer: Merouane...\n",
      "merouane_fixed_v2 | Merouane rank: 1 | Answer: Merouane...\n",
      "mini_test       | Merouane rank: 1 | Answer: Merouane...\n"
     ]
    }
   ],
   "source": [
    "## Cell 20 â€“ RAG PURE DENSE (sans BM25 cassÃ©)\n",
    "\n",
    "def pure_dense_rag(query, indexes, k=50):\n",
    "    \"\"\"RAG pure FAISS (debug version)\"\"\"\n",
    "    results = {}\n",
    "    for strategy, data in indexes.items():\n",
    "        q_emb = model_e.encode([query], normalize_embeddings=True)\n",
    "        scores, indices = data['faiss'].search(q_emb, k)\n",
    "        \n",
    "        retrieved = []\n",
    "        for i in range(k):\n",
    "            idx = indices[0][i]\n",
    "            if idx < len(data['texts']):\n",
    "                retrieved.append((float(scores[0][i]), data['texts'][idx]))\n",
    "        \n",
    "        prompt = build_advanced_prompt(query, retrieved[:10], strategy)\n",
    "        answer = generate_answer(prompt)\n",
    "        \n",
    "        results[strategy] = {'retrieved': retrieved, 'answer': answer}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test pur dense\n",
    "pure_results = pure_dense_rag(\"Who created this RAG notebook?\", indexes, k=50)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"ðŸŽ¯ PURE DENSE RAG (k=50, sans BM25)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for strategy, res in pure_results.items():\n",
    "    merouane_pos = next((i+1 for i, (_, t) in enumerate(res['retrieved']) if \"Merouane\" in t), \"N/A\")\n",
    "    print(f\"{strategy:15} | Merouane rank: {merouane_pos} | Answer: {res['answer'][:60]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
