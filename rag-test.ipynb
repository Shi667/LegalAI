{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9050cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    passages: Dataset({\n",
      "        features: ['passage', 'id'],\n",
      "        num_rows: 3200\n",
      "    })\n",
      "})\n",
      "{'passage': 'Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area.', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "ds_corpus = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
    "passages_ds = ds_corpus[\"passages\"]\n",
    "\n",
    "print(ds_corpus)\n",
    "print(passages_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d797bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de passages: 3200\n",
      "Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part of South America.  It is home to 3.3 million people, of which 1.7 million live in the capital Montevideo and its metropolitan area.\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for ex in passages_ds:\n",
    "    \n",
    "    if isinstance(ex[\"passage\"], list):\n",
    "        texts.extend(ex[\"passage\"])\n",
    "    else:\n",
    "        texts.append(ex[\"passage\"])\n",
    "\n",
    "print(\"Nb de passages:\", len(texts))\n",
    "print(texts[0][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff9e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arist\\miniconda3\\envs\\random\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arist\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 756.86it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embeddings shape: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Modèle d'embedding simple et fiable\n",
    "embed_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "model_e = SentenceTransformer(embed_name)\n",
    "print(\"Embedding model loaded:\", embed_name)\n",
    "\n",
    "# Exemple de test\n",
    "texts_test = [\n",
    "    \"Uruguay is a country in South America.\",\n",
    "    \"Harry Potter is a series of fantasy novels written by J.K. Rowling.\",\n",
    "]\n",
    "\n",
    "embs_test = model_e.encode(texts_test, convert_to_numpy=True, normalize_embeddings=True)\n",
    "print(\"Embeddings shape:\", embs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c3f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 50/50 [00:02<00:00, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index dimension: 384\n",
      "Index size: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optionnel : pour aller plus vite au début, tu peux limiter\n",
    "# texts = texts[:2000]\n",
    "\n",
    "embs = model_e.encode(\n",
    "    texts,\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product sur vecteurs normalisés ≈ cosine\n",
    "index.add(embs)\n",
    "\n",
    "print(\"Index dimension:\", dim)\n",
    "print(\"Index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6e668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.41328710317611694\n",
      "Passage: Grant writing his memoirs. ...\n",
      "\n",
      "Score: 0.40589794516563416\n",
      "Passage: * Davidson, Hugh M. Blaise Pascal. Boston: Twayne Publishers, 1983. ...\n",
      "\n",
      "Score: 0.36122581362724304\n",
      "Passage: *Garland, Hamlin, Ulysses S. Grant: His Life and Character, Macmillan Company, 1898. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search(query, k=5):\n",
    "    # Encode la question\n",
    "    q_emb = model_e.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    # Recherche top-k dans l'index\n",
    "    scores, idx = index.search(q_emb, k)\n",
    "    idx = idx[0].tolist()\n",
    "    scores = scores[0].tolist()\n",
    "    # Retourne une liste (score, passage)\n",
    "    return [(scores[i], texts[idx[i]]) for i in range(len(idx))]\n",
    "\n",
    "# Test\n",
    "test_query = \"Who is the author of Harry Potter?\"\n",
    "retr = search(test_query, k=3)\n",
    "for s, t in retr:\n",
    "    print(\"Score:\", s)\n",
    "    print(\"Passage:\", t[:200], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4548fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 291/291 [00:17<00:00, 16.52it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral + LoRA loaded.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./mistral-rag-mini-lora\")\n",
    "\n",
    "print(\"Mistral + LoRA loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172f9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'top_p', 'temperature', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    out = gen(prompt)[0][\"generated_text\"]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c46875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, retrieved_passages):\n",
    "    ctx_text = \"\\n\\n\".join([p for _, p in retrieved_passages])\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Use ONLY the following context to answer the question.\\n\\n\"\n",
    "        f\"Context:\\n{ctx_text}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9032dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      " You are a helpful assistant. Use ONLY the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "Grant writing his memoirs.\n",
      "\n",
      "* Davidson, Hugh M. Blaise Pascal. Boston: Twayne Publishers, 1983.\n",
      "\n",
      "*Garland, Hamlin, Ulysses S. Grant: His Life and Character, Macmillan Company, 1898.\n",
      "\n",
      "* Farrell, John. \"Pascal and Power\". Chapter seven of Paranoia and Modernity: Cervantes to Rousseau (Cornell UP, 2006).\n",
      "\n",
      "* Williams, L. Pearce (1971), Faraday: A Biography, Simon and Schuster. \n",
      "\n",
      "Question: Who is the author of Harry Potter?\n",
      "\n",
      "Answer: \n",
      "\n",
      "=== ANSWER ===\n",
      " You are a helpful assistant. Use ONLY the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "Grant writing his memoirs.\n",
      "\n",
      "* Davidson, Hugh M. Blaise Pascal. Boston: Twayne Publishers, 1983.\n",
      "\n",
      "*Garland, Hamlin, Ulysses S. Grant: His Life and Character, Macmillan Company, 1898.\n",
      "\n",
      "* Farrell, John. \"Pascal and Power\". Chapter seven of Paranoia and Modernity: Cervantes to Rousseau (Cornell UP, 2006).\n",
      "\n",
      "* Williams, L. Pearce (1971), Faraday: A Biography, Simon and Schuster. \n",
      "\n",
      "Question: Who is the author of Harry Potter?\n",
      "\n",
      "Answer: J. K. Rowling\n"
     ]
    }
   ],
   "source": [
    "def rag_answer(query, k=5, show_prompt=False):\n",
    "    retrieved = search(query, k=k)          # 1) retrieval dense\n",
    "    prompt = build_prompt(query, retrieved) # 2) construction du prompt\n",
    "    answer = generate_answer(prompt)        # 3) génération Mistral\n",
    "    if show_prompt:\n",
    "        print(\"=== PROMPT ===\\n\", prompt, \"\\n\")\n",
    "    return answer, retrieved, prompt\n",
    "\n",
    "# Test\n",
    "q = \"Who is the author of Harry Potter?\"\n",
    "answer, retrieved, prompt = rag_answer(q, k=5, show_prompt=True)\n",
    "\n",
    "print(\"=== ANSWER ===\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c414d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New index size: 3201\n"
     ]
    }
   ],
   "source": [
    "# 1) Ajouter un passage artificiel dans le corpus\n",
    "custom_fact = \"The creator of this notebook is named Merouane.\"\n",
    "texts.append(custom_fact)\n",
    "\n",
    "# 2) Recalculer les embeddings pour ce nouveau passage uniquement\n",
    "import numpy as np\n",
    "\n",
    "new_emb = model_e.encode([custom_fact], convert_to_numpy=True, normalize_embeddings=True)\n",
    "index.add(new_emb)  # on ajoute ce vecteur à l'index FAISS\n",
    "\n",
    "print(\"New index size:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87db3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7658967971801758\n",
      "Passage: The creator of this notebook is named Merouane. \n",
      "\n",
      "Score: 0.4244401752948761\n",
      "Passage: * \" Nikola Tesla\". IEEE History Center, 2005. \n",
      "\n",
      "Score: 0.42050060629844666\n",
      "Passage: * Rybak, James P., \"Nikola Tesla: Scientific Savant\". Popular Electronics, 1042170X, Nov99, Vol. 16, Issue 11. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the name of the creator of this notebook?\"\n",
    "retr = search(q, k=3)\n",
    "for s, t in retr:\n",
    "    print(\"Score:\", s)\n",
    "    print(\"Passage:\", t, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3c567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      " You are a helpful assistant. Use ONLY the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "The creator of this notebook is named Merouane.\n",
      "\n",
      "* \" Nikola Tesla\". IEEE History Center, 2005.\n",
      "\n",
      "* Rybak, James P., \"Nikola Tesla: Scientific Savant\". Popular Electronics, 1042170X, Nov99, Vol. 16, Issue 11.\n",
      "\n",
      "Question: What is the name of the creator of this notebook?\n",
      "\n",
      "Answer: \n",
      "\n",
      "=== ANSWER ===\n",
      " You are a helpful assistant. Use ONLY the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "The creator of this notebook is named Merouane.\n",
      "\n",
      "* \" Nikola Tesla\". IEEE History Center, 2005.\n",
      "\n",
      "* Rybak, James P., \"Nikola Tesla: Scientific Savant\". Popular Electronics, 1042170X, Nov99, Vol. 16, Issue 11.\n",
      "\n",
      "Question: What is the name of the creator of this notebook?\n",
      "\n",
      "Answer: Merouane\n"
     ]
    }
   ],
   "source": [
    "answer, retrieved, prompt = rag_answer(q, k=3, show_prompt=True)\n",
    "print(\"=== ANSWER ===\\n\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
